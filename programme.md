---
layout: page
title: Programme
---

Clarity-2023 will be a one-day workshop with a single track.

The morning will focus on <b>hearing aid speech intelligibility prediction</b> and will present the outcomes 2nd Clarity Prediction Challenge. The afternoon will focus on <b>hearing aid speech enhancement</b> including a presentation of plans for 3rd Clarity Enhancement Challenge.

All sessions will be in the McNabb Lecture Theatre, which is downstairs from the main reception. The first session will start at 9:00am, please arrive in good time to collect your name badge.

Timings and session details are provided below. All times are in Dublin local time (UTC+1).

<div class="panel panel-default">
<div class="panel-body">

<div class="card  m-3">

<div class="card-body">

<table style="margin-left: 1em;">
<tbody>
<tr><td width=50px >9:00</td><td>Welcome</td></tr>
<tr><td>9:10</td><td><a href="#keynote1">Keynote 1</a> - Theme: Intelligibility Prediction - Fei Chen (SUSTech) </td> </tr>
<tr><td>10:00</td><td>The Clarity Prediction Challenge Overview </td></tr>
<tr><td>10:20</td><td>Break - Coffee/Tea</td></tr>
<tr><td>10:40</td><td><a href="#CPC2">Clarity Prediction Challenge Systems</a></td></tr>
<tr><td>12:40</td><td>Prizes and conclusions </td> </tr>
<tr><td>12:50</td><td>Lunch</td></tr>
<tr><td>13:30</td><td>Hearing Aid Speech Enhancement - A user's perspective</td></tr>
<tr><td>13:50</td><td><a href="#keynote2">Keynote 2</a> - Theme: Speech Enhancement - DeLiang Wang (Ohio State University) </td> </tr>
<tr><td>14:50</td><td>Plans for the 3rd Clarity Enhancement Challenge</td></tr>
<tr><td>15:10</td><td>Discussion</td></tr>
<tr><td>15:30</td><td>Break - Coffee/Tea</td></tr>
<tr><td>15:50</td><td><a href="#invited">Hearing Aid Speech Enhancement - Invited Talks</a></td></tr>
<tr><td>17:30</td><td>Close</td></tr>

</tbody>
</table>
</div>
</div>

<h1>Keynote 1</h1>

<div class="card m-3">
  <a name="keynote1"></a>

<div class="card-header">
<div class="row  align-items-center">

<div class="col-sm-3">
<img src="/clarity2023-workshop/assets/images/fei_chen.png" alt="Fei Chen" class="float-left rounded-circle" style="width:100%" />
</div>

<div class="col-sm-9">
<h1 class="lead">Fei Chen <div class="text-muted">SUSTech, China</div></h1>

<h1>Objective speech intelligibility prediction: Insights from human speech perception</h1>

<button class="btn btn-primary" style="color:white; margin: 10px; border-radius: 4px;" type="button" data-toggle="collapse" data-target="#collapseAbstractChen" aria-expanded="false" aria-controls="collapseAbstractChen">
    Abstract and Bio
  </button>

</div>

<div class="collapse" id="collapseAbstractChen">

<div class="card-body">
<h1 class="card-title">Objective speech intelligibility prediction: Insights from human speech perception</h1>

<h3>Abstract</h3>

<p>Speech intelligibility assessment plays an important role in speech and hearing studies. Designing a computational speech intelligibility model can significantly facilitate our studies, e.g., speech enhancement and speech coding. While many objective speech intelligibility prediction models are available, there are still challenges towards improving the prediction performance of intelligibility indices. Human speech perception studies provide us not only knowledge on various (e.g., acoustic, linguistic) impacts on speech understanding in different listening environments, but also insights on design a reliable objective intelligibility prediction index. In this talk, I will first introduce studies on important acoustic cues on human speech perception. Then, I will review the design of some existing intelligibility prediction models and efforts to improve their prediction power. Finally, I will briefly introduce new developments towards objective speech intelligibility prediction, e.g., machine learning and neurophysiological measurement methods.</p>

<h3>Bio</h3>

<p>Fei Chen (Senior Member, IEEE) received the B.Sc. and M.Phil. degrees from the Department of Electronic Science and Engineering, Nanjing University, Nanjing, China, in 1998 and 2001, respectively, and the Ph.D. degree from Department of Electronic Engineering, The Chinese University of Hong Kong, Hong Kong, in 2005. He continued his research as Postdoctor and Senior Research Fellow with the University of Texas at Dallas, supervised by Prof. Philipos Loizou, and The University of Hong Kong, Hong Kong. He is currently a Full Professor with the Department of Electrical and Electronic Engineering, Southern University of Science and Technology (SUSTech), Shenzhen, China. Dr. Chen is leading the speech and physiological signal processing Research Group in SUSTech. He has authored or coauthored more than 100 journal papers and more than 100 conference papers in IEEE journals/conferences, Interspeech, Journal of Acoustical Society of America. His research interests include speech communication and assistive hearing technologies, brain-computer interface, and biomedical signal processing. He was tutorial speaker of Interspeech2022, Interspeech2020, EUSIPCO2022, APSIPA2021, and APSIPA2019, and organized special session Signal processing for assistive hearing devices at ICASSP2015. Dr. Chen is an APSIPA distinguished Lecturer (2022-2023), and is currently an Associate Editor for Biomedical Signal Processing and Control and Frontiers in Human Neuroscience.</p>
</div>

</div>

</div>

</div>
</div>

<a name="CPC2"></a>

<h1>Clarity Prediction Challenge papers</h1>

<div class="card  m-3">
<div class="card-body">

<table>
<tbody>

<!--<div style="margin-bottom: 1em;">Session chair: <i>TBC</i></div>-->

<tr>
<td width=100px valign="top">10:00-10:20</td><td style="padding-bottom: 1em;"><b> The 2nd Clarity Prediction Challenge: A machine learning challenge for hearing aid intelligibility prediction </b> <a href="./slides/Clarity_2023_CPC2_slides_barker.pdf">[PDF]</a>  <a href="https://docs.google.com/presentation/d/1IxmYWlUeu4nxbHykdIWM9_PeIoxwyER2MYW3Q3vUkN4/edit#slide=id.g274425c64e7_1_0">[Google Slides]</a><br /> <span class="author">Jon Barker<sup>1</sup>, Michael A. Akeroyd<sup>2</sup>, Will Bailey<sup>1</sup>, Trevor J. Cox<sup>3</sup>, John F. Culling<sup>4</sup>, Simone Graetzer<sup>3</sup> and Graham Naylor<sup>2</sup></span><i> (<sup>1</sup>University of Sheffield; <sup>2</sup>University of Nottingham; <sup>3</sup>University of Salford; <sup>4</sup>Cardiff University)</i></td>
</tr>

</tbody>
</table>

</div>
</div>

<div class="card  m-3">
<div class="card-body">

<table>
<tbody>

<!--<div style="margin-bottom: 1em;">Session chair: <i>TBC</i></div>-->

<tr>
<td width=100px valign="top">10:40-10:57</td><td style="padding-bottom: 1em;"><b> A Non-Intrusive Speech Intelligibility Prediction Using Binaural Cues and Time-Series Model with One-Hot Listener Embedding </b> <a href="./papers/Clarity_2023_CPC2_paper_mawalim.pdf">[Paper]</a> <br /> <span class="author">Candy Olivia Mawalim, Xiajie Zhou, Shogo Okada, and Masashi Unoki</span><i> (Japan Advanced Institute of Science and Technology, Japan)</i></td>
</tr>

<tr>
<td width=100px valign="top">10:57-11:14</td><td style="padding-bottom: 1em;"><b> Deep Learning-based Speech Intelligibility Prediction Model by Incorporating Whisper for Hearing Aids </b> <a href="./papers/Clarity_2023_CPC2_paper_zezario.pdf">[Paper]</a> <a href="./slides/CPC2_session_Talk_2_zezario.pdf">[Slides]</a> <br /> <span class="author">Ryandhimas E Zezario<sup>1</sup>, Chiou-Shann Fuh<sup>2</sup>, Hsin-Min Wang<sup>1</sup>, Yu Tsao<sup>1</sup>  </span><i> (<sup>1</sup>Academia Sinica, Taiwan; <sup>2</sup>National Taiwan University)</i></td>
</tr>

<tr>
<td width=100px valign="top">11:14-11:31</td><td style="padding-bottom: 1em;"><b> Prediction of Behavioral Speech Intelligibility using a Computational Model of the Auditory System </b> <a href="./papers/Clarity_2023_CPC2_paper_mamun.pdf">[Paper]</a><br /> <span class="author">Nursadul Mamun<sup>1</sup>, Sabbir Ahmed<sup>2</sup>, John.H.L.Hansen<sup>1</sup></span><i> (<sup>1</sup>University of Texas at Dallas, US; <sup>2</sup>Chittagong University of Engineering and Technology, Bangladesh)</i></td>
</tr>

<tr>
<td width=100px valign="top">11:31-11:48</td><td style="padding-bottom: 1em;"><b> Combining Acoustic, Phonetic, Linguistic and Audiometric data in an Intrusive Intelligibility Metric for Hearing-Impaired Listeners </b> <a href="./papers/Clarity_2023_CPC2_paper_huckvale.pdf">[Paper]</a> <a href="./slides/CPC2_session_Talk_4_huckvale.pdf">[Slides]</a><br /> <span class="author">Mark Huckvale and Gaston Hilkhuysen</span><i> (University College London, UK)</i></td>
</tr>

<tr>
<td width=100px valign="top">11:48-12:05</td><td style="padding-bottom: 1em;"><b> A Non-intrusive Binaural Speech Intelligibility Prediction for Clarity-2023 </b> <a href="./papers/Clarity_2023_CPC2_paper_yamamoto.pdf">[Paper]</a> <a href="./slides/CPC2_session_Talk_5_yamamoto.pdf">[Slides]</a><br /> <span class="author">Katsuhiko Yamamoto  </span><i>(AI Lab, CyberAgent, Inc., Japan)</i></td>
</tr>

<tr>
<td width=100px valign="top">12:05-12:22</td><td style="padding-bottom: 1em;"><b> Pre-Trained Intermediate ASR Features and Human Memory Simulation for
Non-Intrusive Speech Intelligibility Prediction in the Clarity Prediction Challenge 2 </b> <a href="./papers/Clarity_2023_CPC2_paper_mogridge.pdf">[Paper]</a> <a href="./slides/CPC2_session_Talk_6_mogridge.pdf">[Slides]</a><br /> <span class="author">Rhiannon Mogridge, George Close, Robert Sutherland, Stefan Goetze and Anton Ragni </span><i> (University of Sheffield, UK)</i></td>
</tr>

<tr>
<td width=100px valign="top">12:22-12:40</td><td style="padding-bottom: 1em;"><b> Temporal-hierarchical features from noise-robust speech foundation models for non-intrusive intelligibility prediction </b><a href="./papers/Clarity_2023_CPC2_paper_cuervo.pdf">[Paper]</a> <a href="./slides/CPC2_session_Talk_7_cuervo.pdf">[Slides]</a><br /> <span class="author">Santiago Cuervo and Ricard Marxer  </span> <i>(Université de Toulon, Aix Marseille Université, France)</i></td>
</tr>

</tbody>
</table>

</div>
</div>

<a name="invited2"></a>

<h1>Invited talk</h1>

<div class="card  m-3">
<div class="card-body">

<table>
<tbody>
<!--<div style="margin-bottom: 1em;">Session chair: <i>TBC</i></div>-->

<tr>
<td width=100px valign="top">15:30-15:50</td><td style="padding-bottom: 1em;"><b> Hearing Aid Speech Enhancement - A User's Perspective </b><br /> <span class="author"><a href="#wong">Emma McAuley</a></span> <i>(Chime, Ireland)</i></td>
</tr>

</tbody>
</table>

</div>
</div>

<h1>Keynote 2</h1>

<div class="card m-3">
  <a name="keynote2"></a>

<div class="card-header">
<div class="row  align-items-center">

<div class="col-sm-3">
<img src="/clarity2023-workshop/assets/images/deliang_wang.png" alt="DeLiang Wang" class="float-left rounded-circle" style="width:100%" />
</div>

<div class="col-sm-9">
<h1 class="lead">DeLiang Wang <div class="text-muted">Ohio State University, US</div></h1>

<h1>Neural Spectrospatial Filtering</h1>

<button class="btn btn-primary" style="color:white; margin: 10px; border-radius: 4px;" type="button" data-toggle="collapse" data-target="#collapseAbstractWang" aria-expanded="false" aria-controls="collapseAbstractWang">
    Abstract and Bio
  </button>

</div>

<div class="collapse" id="collapseAbstractWang">

<div class="card-body">
<h1 class="card-title">Neural Spectrospatial Filtering</h1>

<h3>Abstract</h3>

<p>As the most widely-used spatial filtering approach for multi-channel signal separation,
beamforming extracts the target signal arriving from a specific direction. We present an
emerging approach based on multi-channel complex spectral mapping, which trains a deep
neural network (DNN) to directly estimate the real and imaginary spectrograms of the target
signal from those of the multi-channel noisy mixture. In this all-neural approach, the trained
DNN itself becomes a nonlinear, time-varying spectrospatial filter. How does this conceptually
simple approach perform relative to commonly-used beamforming techniques on different array
configurations and in different acoustic environments? We examine this issue systematically on
speech dereverberation, speech enhancement, and speaker separation tasks. Comprehensive
evaluations show that multi-channel complex spectral mapping achieves very competitive speech
separation results compared to beamforming for different array geometries, and reduces to
monaural complex spectral mapping in single-channel conditions, demonstrating the versatility
of this new approach for multi-channel and single-channel speech separation. In addition, such
an approach is computationally more efficient than popular mask-based beamforming. We
conclude that this neural spectrospatial filter provides a broader approach than traditional and
DNN-based beamforming.</p>

<h3>Bio</h3>

<p>DeLiang Wang received the B.S. degree and the M.S. degree from Peking (Beijing) University and the Ph.D. degree in 1991 from the University of Southern California all in computer science. Since 1991,he has been with the Department of Computer Science & Engineering and the Center for Cognitive and Brain Sciences at The Ohio State University, where he is a Professor and University Distinguished Scholar. He received the U.S. Office of Naval Research Young Investigator Award in 1996, the 2008 Helmholtz Award and 2020 Ada Lovelace Service Award from the International Neural Network Society (INNS), the 2007 Outstanding Paper Award of the IEEE ComputationalIntelligence Society and the 2019 Best Paper Award of the IEEE Signal Processing Society. He is an IEEE Fellow and ISCA Fellow. He currently serves as Co-Editor-in-Chief of Neural Networks, and a member of the INNS Board of Governors.</p>
</div>

</div>

</div>

</div>
</div>

<h1>Clarity Enhancement Challenge plans</h1>

<div class="card  m-3">
<div class="card-body">

<table>
<tbody>
<!--<div style="margin-bottom: 1em;">Session chair: <i>TBC</i></div>-->

<tr>
<td width=100px valign="top">15:30-15:50</td><td style="padding-bottom: 1em;"><b> CEC3 plans and discussion </b><br /> <span class="author">Trevor J. Cox<sup>3</sup>, Jon Barker<sup>1</sup>, Michael A. Akeroyd<sup>2</sup>, Will Bailey<sup>1</sup>,  John F. Culling<sup>4</sup>, Simone Graetzer<sup>3</sup> and Graham Naylor<sup>2</sup></span><i> (<sup>1</sup>University of Sheffield; <sup>2</sup>University of Nottingham; <sup>3</sup>University of Salford; <sup>4</sup>Cardiff University) </i> <a href="./slides/Clarity_2023_CEC3_slides_cox.pdf">[Slides]</a> </td>
</tr>

</tbody>
</table>

</div>
</div>

<a name="invited"></a>

<h1>Invited talks</h1>

<div class="card  m-3">
<div class="card-body">

<table>
<tbody>
<!--<div style="margin-bottom: 1em;">Session chair: <i>TBC</i></div>-->

<tr>
<td width=100px valign="top">15:50-16:15</td><td style="padding-bottom: 1em;"><b> Project Aria: Investigating Ego-Centric Hearing Augmentation </b><br /> <span class="author"><a href="#wong">Daniel Wong</a></span> <i>(Meta Reality Labs, US)</i></td>
</tr>

<tr>
<td width=100px valign="top">16:15-16:40</td><td style="padding-bottom: 1em;"><b> Application of AI-based Signal processing to assistive hearing solutions </b><br /> <span class="author"><a href="#derleth">Peter Derleth</a></span> <i>(Sonova AG, Switzerland)</i></td>
</tr>

<tr>
<td width=100px valign="top">16:40-17:00</td><td style="padding-bottom: 1em;"><b> Voice Conversion for Lombard Speaking Style with Implicit Acoustic Feature Conditioning</b><br /> <span class="author"><a href="#woszczyk">Dominika Woszczyk</a></span> <i>(Imperial College London, UK)</i></td>
</tr>

<tr>
<td width=100px valign="top">17:00-17:20</td><td style="padding-bottom: 1em;"><b> Designing the Audio-Visual Speech Enhancement Challenge (AVSEC) </b><br /> <span class="author"><a href="#aldana">Lorena Aldana</a></span> <i>(University of Edinburgh, UK)</i></td>
</tr>

<tr>
<td width=100px valign="top">17:20-17:30</td><td style="padding-bottom: 1em;"><b> The COG-MHEAR project - Towards cognitively-inspired 5G-IoT enabled, multi-modal Hearing Aids </b> <a href="./slides/Clarity_2023_Invited_Akeroyd.pdf">[Slides]</a>  <br /> <span class="author"><a href="#akeroyd">Michael Akeroyd</a> </span><i>(University of Nottingham, UK)</i></td>
</tr>

</tbody>
</table>

</div>
</div>

<!-- beginning of invited talk block -->

<div class="card m-3">
<a name="wong"></a>
  
<div class="card-header">
<div class="row  align-items-center">
  
<div class="col-sm-3">
<img src="/clarity2023-workshop/assets/images/daniel_wong.png" alt="Daniel Wong" class="float-left rounded-circle" style="width:100%" />
</div>
  
<div class="col-sm-9">
<h1 class="lead">Daniel Wong<div class="text-muted">Meta Reality Labs, US</div></h1>
  
<h1>Project Aria: Investigating Ego-Centric Hearing Augmentation</h1>
  
<button class="btn btn-primary" style="color:white; margin: 10px; border-radius: 4px;" type="button" data-toggle="collapse" data-target="#collapseAbstractWong" aria-expanded="false" aria-controls="collapseAbstractWong">
Abstract
</button>
  
</div>

<div class="collapse" id="collapseAbstractWong">
  
<div class="card-body">
<h1 class="card-title">Project Aria: Investigating Ego-Centric Hearing Augmentation</h1>
  
<h3>Abstract</h3>
  
<p>Augmented reality glasses provide a practical wearable form-factor for speech enhancement that can leverage multi-microphone processing technology and sensor fusion. One application that Meta Reality Labs Research is focusing on is context-aware hearing augmentation in noisy environments. To help tackle this challenge, Project Aria provides a data-gathering platform for investigating the problem space of ego-centric scene understanding, user understanding and speech enhancement. In this talk, I will discuss the platform and some of the most recent work from Meta on ego-centric hearing augmentation.</p>

</div>
  
</div>
  
</div>

</div>
</div>

<!-- end of invited talk block -->

<!-- beginning of invited talk block -->

<div class="card m-3">
<a name="derleth"></a>
  
<div class="card-header">
<div class="row  align-items-center">
  
<div class="col-sm-3">
<img src="/clarity2023-workshop/assets/images/peter_derleth.png" alt="Peter Derleth" class="float-left rounded-circle" style="width:100%" />
</div>
  
<div class="col-sm-9">
<h1 class="lead">Peter Derleth<div class="text-muted">Sonova AG, Switzerland</div></h1>
  
<h1>Application of AI-based Signal processing on assistive hearing solutions</h1>
  
<button class="btn btn-primary" style="color:white; margin: 10px; border-radius: 4px;" type="button" data-toggle="collapse" data-target="#collapseAbstractDerleth" aria-expanded="false" aria-controls="collapseAbstractDerleth">
Abstract
</button>
  
</div>

<div class="collapse" id="collapseAbstractDerleth">
  
<div class="card-body">
<h1 class="card-title">Application of AI-based Signal processing on assistive hearing solutions</h1>
  
<h3>Abstract</h3>
  
<p>Assistive hearing solutions come in a variety of form factors, are designed to serve various use cases, are targeted at different user groups and are distributed to the market as consumer or medical product. Each of the mentioned aspects influences if a technological/functional innovation reaches the respective market segment and get’s the chance to improve the daily life of human listeners. The presentation will shed a light on existing and near future (AI-based) hearing aid technology.</p>

<h3>Bio</h3>
Dr. Peter Derleth (*1968); Degree in applied physics (1995); PhD in Psychoacoustics (1999) University of Oldenburg, Germany.

Since 2000 employed at Sonova AG, Switzerland. Position: Principal Expert ‘Hearing Performance’
which covers the fields of acoustics, audiology, algorithmic research and performance profiling.
Research topics range from acoustic stability enhancement (Feedback Cancelling) over directional
(Beam Forming) and spectral algorithms (Gain Models, Noise Cancelling, Frequency Manipulations)
to binaural and psychoacoustic effects. Latest focus field covers applications of AI-based Signal-
processing for improved Hearing Performance.

</div>
  
</div>
  
</div>

</div>
</div>

<!-- end of invited talk block -->

<!-- beginning of invited talk block -->

<div class="card m-3">
<a name="woszczyk"></a>
  
<div class="card-header">
<div class="row  align-items-center">
  
<div class="col-sm-3">
<img src="/clarity2023-workshop/assets/images/dominika_woszczyk.png" alt="Dominika Woszczyk" class="float-left rounded-circle" style="width:100%" />
</div>
  
<div class="col-sm-9">
<h1 class="lead">Dominika Woszczyk<div class="text-muted">Imperial College London, UK</div></h1>
  
<h1>Voice Conversion for Lombard Speaking Style with Implicit Acoustic Feature Conditioning</h1>
  
<button class="btn btn-primary" style="color:white; margin: 10px; border-radius: 4px;" type="button" data-toggle="collapse" data-target="#collapseAbstractWoszczyk" aria-expanded="false" aria-controls="collapseAbstractWoszczyk">
Abstract
</button>
  
</div>

<div class="collapse" id="collapseAbstractWoszczyk">
  
<div class="card-body">
<h1 class="card-title">Voice Conversion for Lombard Speaking Style with Implicit Acoustic Feature Conditioning</h1>
  
<i>Dominika C Woszczyk (Imperial College London); Sam Ribeiro (Amazon Alexa); Thomas Merritt (Amazon); Daniel Korzekwa (Nvidia)</i>

<h3>Abstract</h3>
  
<p>Lombard speaking style in Text-to-Speech (TTS) systems can enhance speech intelligibility and be advantageous in noisy environments and for individuals with hearing loss. However, training such models requires a large amount of data and the Lombard effect is challenging to record due to speaker and noise variability and tiring recording conditions. Voice conversion (VC) has been shown to be a useful augmentation technique to train TTS systems when data from the target speaker in the desired speaking style is unavailable. Our focus in this study is on Lombard speaking style conversion, aiming to convert speaker identity while retaining the distinctive acoustic characteristics of the Lombard style. We compare voice conversion models with implicit and explicit acoustic feature conditioning. Our results show that our implicit conditioning strategy achieves an intelligibility gain comparable to the model conditioned on explicit acoustic features, while also preserving speaker similarity.</p>

</div>
  
</div>
  
</div>

</div>
</div>

<!-- end of invited talk block -->

<!-- beginning of invited talk block -->

<div class="card m-3">
<a name="aldana"></a>
  
<div class="card-header">
<div class="row  align-items-center">
  
<div class="col-sm-3">
<img src="/clarity2023-workshop/assets/images/lorena_aldana.png" alt="Lorena Aldana" class="float-left rounded-circle" style="width:100%" />
</div>
  
<div class="col-sm-9">
<h1 class="lead">Lorena Aldana<div class="text-muted">University of Edinburgh, UK</div></h1>
  
<h1>Designing the Audio-Visual Speech Enhancement Challenge (AVSEC)</h1>
  
<button class="btn btn-primary" style="color:white; margin: 10px; border-radius: 4px;" type="button" data-toggle="collapse" data-target="#collapseAbstractAldana" aria-expanded="false" aria-controls="collapseAbstractAldana">
Abstract
</button>
  
</div>

<div class="collapse" id="collapseAbstractAldana">
  
<div class="card-body">
<h1 class="card-title">Designing the Audio-Visual Speech Enhancement Challenge (AVSEC)</h1>
  
<h3>Abstract</h3>
  
<p>The Audio-Visual Speech Enhancement Challenge (AVSEC) sets the first benchmark in the field of audio-visual speech enhancement, providing a carefully designed dataset and scalable protocol for human listening evaluation of AV-SE systems. AV scenes comprise audio and video of a target speaker mixed with an interferer that can be either noise or a competing speaker. Target speaker videos are selected from LRS3. AV-SE systems are evaluated in terms of intelligibility from listening tests with human participants. To evaluate the systems, we propose a scalable and efficient method to assess intelligibility from “in-the-wild stimuli” that does not require a specific sentence structure. This talk will present the scope and limitations of current design choices in AVSEC.</p>

</div>
  
</div>
  
</div>

</div>
</div>

<!-- end of invited talk block -->

<!-- beginning of invited talk block -->

<div class="card m-3">
<a name="akeroyd"></a>
  
<div class="card-header">
<div class="row  align-items-center">
  
<div class="col-sm-3">
<img src="/clarity2023-workshop/assets/images/michael_akeroyd.jpg" alt="Michael Akeroyd" class="float-left rounded-circle" style="width:100%" />
</div>
  
<div class="col-sm-9">
<h1 class="lead">Michael Akeroyd<div class="text-muted">University of Nottingham, UK</div></h1>
  
<h1>The COG-MHEAR project - Towards cognitively-inspired 5G-IoT enabled, multi-modal Hearing Aids</h1>
  
<button class="btn btn-primary" style="color:white; margin: 10px; border-radius: 4px;" type="button" data-toggle="collapse" data-target="#collapseAbstractAkeroyd" aria-expanded="false" aria-controls="collapseAbstractAkeroyd">
Abstract
</button>

</div>

<div class="collapse" id="collapseAbstractAkeroyd">
  
<div class="card-body">
<h1 class="card-title">The COG-MHEAR project - Towards cognitively-inspired 5G-IoT enabled, multi-modal Hearing Aids</h1>
  
<i>Michael A Akeroyd (University of Nottingham), Amir Hussain (Edinburgh Napier), Peter Bell (Edinburgh), Ahsan Adeel (Wolverhampton), Qammar Hussain Abbasi (Glasgow), Steve Renals (Edinburgh), Tughrul Arslan (Edinburgh), Tharmalingam Ratnarajah (Edinburgh), Lynne Baillie (Heriot-Watt), Mathini Sellathurai (Heriot-Watt) , Muhammad Imran (Glasgow), Emma Hart, (Edinburgh Napier), Ahmed Al-Dubai, (Edinburgh Napier), William Buchanan (Edinburgh Napier), Alexander Casson (Manchester), & Dorothy Hardy (Edinburgh Napier)</i>

<h3>Abstract</h3>
  
<p>The lack of take-up of hearing aids, their use, their stigma, the effort required to use them, and the limitations in what they can do for speech enhancement remain fundamental problems for auditory research. The COG-MHEAR project is a 4-year EPSRC-funded project that is taking a transformative, interdisciplinary approach to address some of these issues. We are creating prototypes of multi-modal aids which not only amplify sounds but also use information collected from a range of sensors to improve understanding of speech, including visual information of the movements of the speaker's lips, hand gestures, and similar. But such devices bring challenges in preserving privacy and operating with minimum power and minimum delay. In this talk we will give an overview of the project, some of the results, and some of the challenges.</p>

</div>

</div>
  
</div>

</div>
</div>

<!-- end of invited talk block -->

</div>

<br/>
