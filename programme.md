---
layout: page
title: Programme
---

Clarity-2023 will be a one-day workshop with a single track.

The morning will focus on <b>hearing aid speech intelligibility prediction</b> and will present the outcomes 2nd Clarity Prediction Challenge. The afternoon will focus on <b>hearing aid speech enhancement</b> and will conclude with a discussion of requirements for the 3rd Clarity Enhancement Challenge.

Provisional timings and session details are provided below. All times are in Dublin local time (UTC+1).

<div class="panel panel-default">
<div class="panel-body">

<div class="card  m-3">

<div class="card-body">

<table style="margin-left: 1em;">
<tbody>
<tr><td>9:00</td><td>Welcome</td></tr>
<tr><td>9:10</td><td><a href="#keynote1">Keynote 1</a> - Fei Chen (SUSTech) </td> </tr>
<tr><td>10:00</td><td>The Clarity Prediction Challenge Overview </td></tr>
<tr><td>10:20</td><td>Break</td></tr>
<tr><td>10:40</td><td>Clarity Prediction Challenge Systems</td></tr>
<tr><td>12:40</td><td> Prizes and conclusions </td> </tr>
<tr><td>12:50</td><td>Lunch</td></tr>
<tr><td>13:30</td><td><a href="#keynote2">Keynote 2</a> - DeLiang Wang (Ohio State University) </td> </tr>
<tr><td>14:30</td><td>Break</td></tr>
<tr><td>15:00</td><td>Hearing Aid Speech Enhancement </td></tr>
<tr><td>17:00</td><td> CEC3 discussion + Future Directions </td></tr>
<tr><td>17:30</td><td>Close</td></tr>

</tbody>
</table>
</div>
</div>

<h1>Keynote talks</h1>

<div class="card m-3">
  <a name="keynote1"></a>

<div class="card-header">
<div class="row  align-items-center">

<div class="col-sm-3">
<img src="/clarity2023-workshop/assets/images/fei_chen.png" alt="Fei Chen" class="float-left rounded-circle" style="width:100%" />
</div>

<div class="col-sm-9">
<h1 class="lead">Fei Chen <div class="text-muted">SUSTech, China</div></h1>

<h1>Objective speech intelligibility prediction: Insights from human speech perception</h1>

<button class="btn btn-primary" style="color:white; margin: 10px; border-radius: 4px;" type="button" data-toggle="collapse" data-target="#collapseAbstractWang" aria-expanded="false" aria-controls="collapseAbstractChen">
    Abstract and Bio
  </button>

</div>

<div class="collapse" id="collapseAbstractChen">

<div class="card-body">
<h1 class="card-title">Objective speech intelligibility prediction: Insights from human speech perception</h1>

<h3>Abstract</h3>

<p>Speech intelligibility assessment plays an important role in speech and hearing studies. Designing a computational speech intelligibility model can significantly facilitate our studies, e.g., speech enhancement and speech coding. While many objective speech intelligibility prediction models are available, there are still challenges towards improving the prediction performance of intelligibility indices. Human speech perception studies provide us not only knowledge on various (e.g., acoustic, linguistic) impacts on speech understanding in different listening environments, but also insights on design a reliable objective intelligibility prediction index. In this talk, I will first introduce studies on important acoustic cues on human speech perception. Then, I will review the design of some existing intelligibility prediction models and efforts to improve their prediction power. Finally, I will briefly introduce new developments towards objective speech intelligibility prediction, e.g., machine learning and neurophysiological measurement methods.</p>

<h3>Bio</h3>

<p>Fei Chen (Senior Member, IEEE) received the B.Sc. and M.Phil. degrees from the Department of Electronic Science and Engineering, Nanjing University, Nanjing, China, in 1998 and 2001, respectively, and the Ph.D. degree from Department of Electronic Engineering, The Chinese University of Hong Kong, Hong Kong, in 2005. He continued his research as Postdoctor and Senior Research Fellow with the University of Texas at Dallas, supervised by Prof. Philipos Loizou, and The University of Hong Kong, Hong Kong. He is currently a Full Professor with the Department of Electrical and Electronic Engineering, Southern University of Science and Technology (SUSTech), Shenzhen, China. Dr. Chen is leading the speech and physiological signal processing Research Group in SUSTech. He has authored or coauthored more than 100 journal papers and more than 100 conference papers in IEEE journals/conferences, Interspeech, Journal of Acoustical Society of America. His research interests include speech communication and assistive hearing technologies, brain-computer interface, and biomedical signal processing. He was tutorial speaker of Interspeech2022, Interspeech2020, EUSIPCO2022, APSIPA2021, and APSIPA2019, and organized special session Signal processing for assistive hearing devices at ICASSP2015. Dr. Chen is an APSIPA distinguished Lecturer (2022-2023), and is currently an Associate Editor for Biomedical Signal Processing and Control and Frontiers in Human Neuroscience.</p>
</div>

</div>

</div>

</div>
</div>

<div class="card m-3">
  <a name="keynote2"></a>

<div class="card-header">
<div class="row  align-items-center">

<div class="col-sm-3">
<img src="/clarity2023-workshop/assets/images/deliang_wang.png" alt="DeLiang Wang" class="float-left rounded-circle" style="width:100%" />
</div>

<div class="col-sm-9">
<h1 class="lead">DeLiang Wang <div class="text-muted">Ohio State University, US</div></h1>

<h1>Neural Spectrospatial Filtering</h1>

<button class="btn btn-primary" style="color:white; margin: 10px; border-radius: 4px;" type="button" data-toggle="collapse" data-target="#collapseAbstractWang" aria-expanded="false" aria-controls="collapseAbstractWang">
    Abstract and Bio
  </button>

</div>
</div>
</div>

<div class="collapse" id="collapseAbstractWang">

<div class="card-body">
<h1 class="card-title">Neural Spectrospatial Filtering</h1>

<h3>Abstract</h3>

<p>As the most widely-used spatial filtering approach for multi-channel signal separation,
beamforming extracts the target signal arriving from a specific direction. We present an
emerging approach based on multi-channel complex spectral mapping, which trains a deep
neural network (DNN) to directly estimate the real and imaginary spectrograms of the target
signal from those of the multi-channel noisy mixture. In this all-neural approach, the trained
DNN itself becomes a nonlinear, time-varying spectrospatial filter. How does this conceptually
simple approach perform relative to commonly-used beamforming techniques on different array
configurations and in different acoustic environments? We examine this issue systematically on
speech dereverberation, speech enhancement, and speaker separation tasks. Comprehensive
evaluations show that multi-channel complex spectral mapping achieves very competitive speech
separation results compared to beamforming for different array geometries, and reduces to
monaural complex spectral mapping in single-channel conditions, demonstrating the versatility
of this new approach for multi-channel and single-channel speech separation. In addition, such
an approach is computationally more efficient than popular mask-based beamforming. We
conclude that this neural spectrospatial filter provides a broader approach than traditional and
DNN-based beamforming.</p>

<h3>Bio</h3>

<p>DeLiang Wang received the B.S. degree and the M.S. degree from Peking (Beijing) University and the Ph.D. degree in 1991 from the University of Southern California all in computer science. Since 1991,he has been with the Department of Computer Science & Engineering and the Center for Cognitive and Brain Sciences at The Ohio State University, where he is a Professor and University Distinguished Scholar. He received the U.S. Office of Naval Research Young Investigator Award in 1996, the 2008 Helmholtz Award and 2020 Ada Lovelace Service Award from the International Neural Network Society (INNS), the 2007 Outstanding Paper Award of the IEEE ComputationalIntelligence Society and the 2019 Best Paper Award of the IEEE Signal Processing Society. He is an IEEE Fellow and ISCA Fellow. He currently serves as Co-Editor-in-Chief of Neural Networks, and a member of the INNS Board of Governors.</p>
</div>

</div>

</div>

<br/>
