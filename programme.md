---
layout: page
title: Programme
---

Timings and session details are provided below. All times are UK local time (i.e., <b>UTC</b>).

<div class="panel panel-default">
<div class="panel-body">

<div class="card  m-3">

<div class="card-body">

<table style="margin-left: 1em;">
<tbody>
<tr><td>9:00</td><td>Welcome  <b><a href=""> <!--[slides]--></a></b></td><td><a href="https://youtu.be/fY4auzYzxX0">[YouTube]</a></td></tr>
<tr><td>9:10</td><td>The Clarity CEC2 Overview<b><a href=""><!--[slides]--></a></b> </td><td><a href="https://youtu.be/YI9_V0b6uyk">[YouTube]</a></td></tr>
<tr><td>9:40</td><td><span class="bold"><a href="#session1">Challenge Papers: Session I</a></span></td></tr>
<tr><td>11:00</td><td>Break</td></tr>
<tr><td>11:30</td><td><a href="#keynote">Invited Talk</a> - Karolina Smeds (ORCA Europe) and Stefan Petrausch (WS Audiology) </td> <td><a href="https://youtu.be/hZcdCVkP-KI">[YouTube]</a></td></tr>
<tr><td>12:00</td><td> CPC2 and CEC3 discussion + Future Directions </td></tr>
<tr><td>12:30</td><td>Break</td></tr>
<tr><td>13:00</td><td><span class="bold"><a href="#session2">Challenge Papers: Session II</a></span></td></tr>
<tr><td>14:20</td><td> Prizes and conclusions  <!---<b><a href="./presentations/clarity2022_prizes.pdf">[slides]</a></b>---></td><td><a href="https://youtu.be/E_WgCxRviUs">[YouTube]</a></td></tr>
<tr><td>14:40</td><td>Close</td></tr>

</tbody>
</table>
</div>
</div>

Links to technical reports and videos for all talks are provided in the programme below.

<h1>Invited Talk</h1>

<div class="card m-3 mt-4">
  <a name="keynote"></a>

<div class="card-header">
<div class="row align-items-center">

<!--<div class="col-sm-3">
<img src="./assets/images/smeds-photo.png" alt="Karolina Smeds" class="float-left rounded-circle" style="width:100%" />
</div>-->

<div class="col-sm-2">
<img src="./assets/images/smeds.png" alt="Karolina Smeds" class="float-left" style="width:80%; height:80%;" />
</div>

<div class="col-sm-2">
<h1 class="lead">Karolina Smeds <div class="text-muted">ORCA Europe<br>WS Audiology</div> </h1>
</div>

<div class="col-sm-2">
<img src="./assets/images/petrausch2.png" alt="Stefan Petrausch" class="float-left" style="width:80%; height:80%;" />
</div>

<div class="col-sm-2">
<h1 class="lead">Stefan Petrausch <div class="text-muted">WS Audiology</div> </h1>
</div>

<div class="col-sm-4">

<h1>Real-life listening through hearing aids.</h1>

<button class="btn btn-primary" style="color:white; margin: 10px; border-radius: 4px;" type="button" data-toggle="collapse" data-target="#collapseAbstractSmeds" aria-expanded="false" aria-controls="collapseAbstractSmeds">
    Abstract and Bio

  </button>

<!--
<a href="https://youtu.be/waPONoYrf8Q;feature=youtu.be" class="btn btn-primary" style="color:white; margin: 10px; border-radius: 4px;" type="button">
    YouTube
  </a>

<a href="./presentations/Clarity_2021_slides_tutorial_smeds.pdf" class="btn btn-primary" style="color:white; margin: 10px; border-radius: 4px;" type="button">
    Slides
  </a>
-->

</div>
</div>
</div>

<!---------------------------------------------------->
<div class="collapse" id="collapseAbstractSmeds">

<div class="card-body">
<h1 class="card-title">Real-life listening through hearing aids (Karolina Smeds and Stefan Petrausch)</h1>

<h2>Abstract</h2>

<p>The presentation will introduce “real-life listening” and describe one particular listening situation, often described as difficult by hearing-aid users: the group conversation. Some of the special characteristics of a group conversation will be presented and ways to evaluate success in group conversations will be mentioned. In the second part of the presentation, typical challenges for machine learning applications in this context of “real-life listening” will be discussed. Requirements on robustness, sound quality, latency, and computational complexity constitute certain boundary conditions, which have to be considered right from the start of the development. It will be shown how current applications, like acoustic classification and own voice detection, and current research activities are dealing with these restrictions.</p>

<h2>Bios</h2>

<p><b>Karolina Smeds</b> has a background in Engineering Physics and Audiology. Her PhD work focused on loudness aspects of hearing-aid fitting, combining clinical and theoretical aspects on the topic. For 15 years, Karolina led an external research group, ORCA Europe in Stockholm, Sweden, fully funded by the Danish hearing-aid manufacturer Widex A/S, now WS Audiology, where she still works. Recently the research group has focused on investigations of “real-life hearing”. This includes investigations of people’s auditory reality and development and evaluation of outcome measures for hearing-aid fitting success, both in the laboratory and in the field, that can produce results that are indicative of real-life performance and preference. Recently, the group has moved into the field of health psychology and into investigations of spoken conversations. At the University of Nottingham, Karolina is continuing to work on auditory reality, outcome measures that can produce ecologically valid results, and analysis of spoken conversations, primarily in collaboration with the Scottish Section of the Hearing Sciences group.</p>

<p><b>Stefan Petrausch</b> studied electrical engineering at the University of Erlangen-Nuremberg, where he received his diploma degree (Dipl.-Ing.) and his doctoral degree (Dr.-Ing.) in the year 2002 and 2007 respectively. He performed his PhD thesis with the title &quot;Block-Based Physical Modeling&quot; in the field of musical signal processing, dealing with distributed methods for the numerical simulation of partial differential equations. Since 2007 he is a member of the signal processing group at WSAudiology, where he is currently leading the team and activities for research centered signal processing development and prototype applications. In this context, he has worked on almost all signal processing aspects for digital hearing instruments like directional processing, adaptive feedback cancellation, and binaural signal processing, with more and more focus on machine learning solutions for these topics in the last years.</p>

</div>
</div>
<!---------------------------------------------------->

</div>
</div>

<a name="session1"></a>

Challenge paper sessions will consist of oral presentations allowing 15 minutes per team and 5 minutes for Q&A.

<h1>Challenge Papers: Session I</h1>

<div class="card  m-3">
<div class="card-body">

<table>
<tbody>

<!--<div style="margin-bottom: 1em;">Session chair: <i>TBC</i></div>-->

<!--Sheffield E032-->
<tr><td valign="top" style="margin-right: 5em;"></td>
<td><b>	Sheffield System for the Second Clarity Enhancement Challenge <a href="papers/Clarity_2022_CEC2_paper_tu.pdf">[Report]</a> <a href="https://youtu.be/eQNKEPhwAfQ">[YouTube]</a> </b><br /> <span class="author">Zehai Tu<sup>1</sup>, Jisi Zhang<sup>1</sup>, Ning Ma<sup>1</sup>, Jon Barker<sup>1</sup></span><i>(<sup>1</sup>University of Sheffield, Department of Computer Science, Sheffield, UK)</i></td>
</tr>

<!--Oldenburg E036 E038-->
<tr><td valign="top" style="margin-right: 5em;"></td>
<td><b>	Informed Target Speaker Extraction Using TCN and TCN-Conformer Architectures for the 2nd Clarity Enhancement Challenge <a href="papers/Clarity_2022_CEC2_paper_tammen.pdf">[Report]</a> <a href="https://youtu.be/hFtx3PTs5HI">[YouTube]</a></b> <br /> <span class="author">Marvin Tammen<sup>1</sup>, Ragini Sinha<sup>2</sup>, Henri Gode<sup>1</sup>, Daniel Fejgin<sup>1</sup>, Wiebke Middelberg<sup>1</sup>, Eike J. Nustede<sup>1</sup>, Reza Varzandeh<sup>1</sup>, Jörn Anemüller<sup>1</sup>, Gerald Enzner<sup>1</sup> and Simon Doclo<sup>1,2</sup></span><i> (<sup>1</sup>Department of Medical Physics and Acoustics and Cluster of Excellence Hearing4all, University of Oldenburg, Germany, <sup>2</sup>Fraunhofer Institute for Digital Media Technology IDMT, Oldenburg Branch for Hearing, Speech and Audio Technology HSA, Germany)</i></td>
</tr>

<!--Sinica-->
<tr><td valign="top" style="margin-right: 5em;"></td>
<td><b>	CITEAR: A Two-Stage End-to-End System for Noisy-Reverberant Hearing-Aid Processing <a href="papers/Clarity_2022_CEC2_paper_lee.pdf">[Report]</a> <a href="https://youtu.be/0UovL5jwMhU">[YouTube]</a></b><br /> <span class="author">Chi-Chang Lee<sup>1,2</sup>, Hong-Wei Chen<sup>3</sup>, Rong Chao<sup>2,4</sup>, Tsung-Te Liu<sup>3</sup>, Yu Tsao<sup>2</sup></span><i>(<sup>1</sup>Department of Computer Science and Information Engineering, National Taiwan University, Taipei, Taiwan,
<sup>2</sup>Research Center for Information Technology Innovation, Academia Sinica, Taipei, Taiwan,
<sup>3</sup>Graduate Institute of Electronics Engineering, National Taiwan University, Taipei, Taiwan,
<sup>4</sup>Department of Computer Science and Information Engineering,National Cheng-Kung University, Tainan, Taiwan)</i></td>
</tr>

<!--Nanjing-->
<tr><td valign="top" style="margin-right: 5em;"></td>
<td><b>	LNSFEars: Low-latency Neural Spectrospatioal Filtering and Equalizer for hearing aids <a href="papers/Clarity_2022_CEC2_paper_lei.pdf">[Report]</a> <a href="https://youtu.be/fYGWE2881vM">[YouTube]</a></b><br /> <span class="author">Tong Lei<sup>1,2</sup>, Zhongshu Hou<sup>1,2</sup>, Yuxiang Hu<sup>2</sup>, Wanyu Yang<sup>2</sup>, Tianchi Sun <sup>1,2</sup>, Xiaobin Rong <sup>1,2</sup>, Dahan Wang <sup>1,2</sup>, and Jing Lu <sup>1,2</sup></span><i>(<sup>1</sup>Key Laboratory of Modern Acoustics Institute of Acoustics, Nanjing University, Nanjing 210093,

China

<sup>2</sup> NJU-Horizon Intelligent Audio Lab, Horizon Robotics, Nanjing 210038, China.)</i></td>

</tr>

</tbody>
</table>

</div>
</div>

<a name="session2"></a>

<h1>Challenge Papers: Session II</h1>

<div class="card  m-3">
<div class="card-body">

<table>
<tbody>

<!--<div style="margin-bottom: 1em;">Session chair: <i>TBC</i></div>-->

<!--Oldenburg E019-->
<tr><td valign="top" style="margin-right: 5em;"></td>
<td><b>	LLMSE: Low-latency Multi-channel Speech Enhancement for Hearing Aids <a href="papers/Clarity_2022_CEC2_paper_ouyang.pdf">[Report]</a> <a href="https://youtu.be/OVryFiL_zUc">[YouTube]</a></b><br /> <span class="author">Chengwei Ouyang, Kexin Fei, Haoshuai Zhou, Linkai Li, Congxi Lu</span><i> (Orka Inc.)</i></td>
</tr>

<!--Inner Mongolia E008-->
<tr><td valign="top" style="margin-right: 5em;"></td>
<td><b>	DRC-NET for The 2nd Clarity Enhancement Challenge <a href="papers/Clarity_2022_CEC2_paper_liu.pdf">[Report]</a> <a href="https://youtu.be/FZ0Oog_qf9E">[YouTube]</a></b><br /> <span class="author">Jinjiang Liu, Xueliang Zhang</span><i> (College of Computer Science, Inner Mongolia University, China)</i></td>
</tr>

<!--CMU-->
<tr><td valign="top" style="margin-right: 5em;"></td>
<td><b>	Multi-channel Target Speaker Extraction with Refinement: The WAVLAB Submission to the Second Clarity Enhancement Challenge  <a href="papers/Clarity_2022_CEC2_paper_cornell.pdf">[Report]</a> <a href="https://youtu.be/dO0eFvzvT4M">[YouTube]</a>  </b> <br /> <span class="author">Samuele Cornell<sup>1,2</sup>, Zhong-Qiu Wang<sup>2</sup>, Yoshiki Masuyama<sup>3,2</sup>, Shinji Watanabe<sup>2</sup>Manuel Pariente<sup>4</sup>, Nobutaka Ono<sup>3</sup></span><i> (<sup>1</sup>Universita Politecnica delle Marche, Italy,
<sup>2</sup>Carnegie Mellon University, USA,
<sup>3</sup>Tokyo Metropolitan University, Japan 4Pulse Audition, France)</i></td>
</tr>

</tbody>
</table>

</div>
</div>

</div>
